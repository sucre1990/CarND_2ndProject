{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Traffic Sign Recognition** \n",
    "**Build a Traffic Sign Recognition Project**\n",
    "The goals / steps of this project are the following:\n",
    "* Load the data set (see below for links to the project data set)\n",
    "* Explore, summarize and visualize the data set\n",
    "* Design, train and test a model architecture\n",
    "* Use the model to make predictions on new images\n",
    "* Analyze the softmax probabilities of the new images\n",
    "* Summarize the results with a written report\n",
    "\n",
    "\n",
    "## Rubric Points\n",
    "### Here I will consider the [rubric points](https://review.udacity.com/#!/rubrics/481/view) individually and describe how I addressed each point in my implementation. \n",
    "\n",
    "**Build a Traffic Sign Recognition Project**\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "* Load the data set (see below for links to the project data set)\n",
    "* Explore, summarize and visualize the data set\n",
    "* Design, train and test a model architecture\n",
    "* Use the model to make predictions on new images\n",
    "* Analyze the softmax probabilities of the new images\n",
    "* Summarize the results with a written report\n",
    "\n",
    "[image1]: ./pictures_for_writeup/3_test_signs.png \"3 Traffic Sign\"\n",
    "[image2]: ./pictures_for_writeup/training_distribution.png \"training distribution\"\n",
    "[image3]: ./pictures_for_writeup/validation_distribution.png \"validation distribution\"\n",
    "[image4]: ./pictures_for_writeup/testing_distribution.png \"testing distribution\"\n",
    "[image5]: ./pictures_for_writeup/Before_converted_and_normalize.png \"before convert\"\n",
    "[image6]: ./pictures_for_writeup/After_converted_and_normalize.png \"after convert\"\n",
    "[image7]: ./pictures_for_writeup/1-Figure2-1.png \"CNN architecture\"\n",
    "\n",
    "---\n",
    "### Writeup / README\n",
    "You're reading it! and here is a link to my [project code](https://github.com/sucre1990/CarND_2ndProject/blob/master/Traffic_Sign_Classifier.ipynb)\n",
    "\n",
    "### Data Set Summary & Exploration\n",
    "\n",
    "#### 1. Provide a basic summary of the data set.\n",
    "\n",
    "The content for this section is in cell 2 of the python notebook and here are some statistics:\n",
    "\n",
    "* Number of training examples = 34799\n",
    "* Number of validation examples = 4410\n",
    "* Number of testing examples = 12630\n",
    "* Image data shape = (32, 32, 3)\n",
    "* Number of classes = 43\n",
    "\n",
    "#### 2. Include an exploratory visualization of the dataset.\n",
    "Here is an exploratory visualization of the data set.\n",
    "* First is 3 random pictures from training dataset and I used matplotlib to plot the pictures.\n",
    "\n",
    "\n",
    "![3 random testing signs][image1]\n",
    "\n",
    "* Hist graphs below show the traffic sign distribution among training data, validation data and testing data\n",
    "\n",
    "![training distribution][image2]\n",
    "\n",
    "![validation distribution][image3]\n",
    "\n",
    "![testing distribution][image4]\n",
    "**As we can see from the three distrbution graphs. We have uneven data both for training, validating and testing. It may bring us problems**\n",
    "\n",
    "### Design and Test a Model Architecture\n",
    "#### 1. Describe how you preprocessed the image data.\n",
    "\n",
    "*I got inspired by Pierre Sermanet and Yann LeCun's \"Traffic Sign Recognition with Multi-Scale Convolutional Networks\"*\n",
    "**Image Augmentation**\n",
    "As we see from the data exploration, the training data is not evenly distributed. For better result, I added augumentation for the training set (every catagory reached around 2200 samples, augmentation methods: randomly adopts horiztally flipping, adding Gaussian noise and random dropout)\n",
    "\n",
    "**Reason for Image Augmentation**\n",
    "1. balance the training set\n",
    "2. increase the total training sample\n",
    "\n",
    "Here is the new distribution:\n",
    "[image2.1]:./pictures_for_writeup/Augmented_training_distribution.png \"Augmented training distribution\"\n",
    "\n",
    "![][image2.1]\n",
    "**Image Processing (data preprocessing)**\n",
    "\n",
    "As the first step, I converted the RGB format to gray and normalized it. (Instead of converting RGB to YUV space and using UV channel like LeCun's work, I chose to use only Y channel which is the grayscale channel).\n",
    "\n",
    "**Reason for Converting to Grayscale and Normalization **\n",
    "\n",
    "1. Converting grayscale is to better capture the content in the picture and reduce irrelavant information.\n",
    "\n",
    "2. Normalizing grayscale picture is for training the CNN. Since the SGD algorithm relys on backward propagation to update weigths throughout the network as training examples are passed through. If we didn't scale our input values, some of the feature values would be very likely different from others' and this would cause weights updated not evenly (some over compensated and some under corrected) and hard to find opitmization point.\n",
    "\n",
    "Here is an example of a traffic sign image before and after grayscaling.\n",
    "![][image5]\n",
    "![][image6]\n",
    "After grayscaling them, I applied a normalization to the picture as the CNN requires normalization. \n",
    "You can find **CVT_Gray_Norm** function is dealing with those two steps.\n",
    "\n",
    "*Improvements in the future: There are no more data processing after normaliztion, but I did realize there are some pictures are too dark and in the future I will consider additional processes.*\n",
    "\n",
    "#### 2. Describe what your final model architecture looks like including model type, layers, layer sizes, connectivity, etc.)\n",
    "\n",
    "Here I used a picture from [Pierre Sermanet and Yann LeCun's Traffic Sign Recognition with Multi-Scale Convolutional Networks (http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf) to illustrate my CNN architecture.\n",
    "![][image7]\n",
    "\n",
    "\n",
    "| Layer         \t\t|     Description\t        \t\t\t\t\t            | \n",
    "|:---------------------:|:---------------------------------------------------------:| \n",
    "| Input         \t\t| 32x32x1 RGB image   \t\t\t\t\t\t\t            | \n",
    "| Convolution 5x5     \t| 1x1 stride, same padding, 30 filters, outputs 28x28x30   \t|\n",
    "| RELU\t\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t            |\n",
    "| Max pooling\t      \t| 2x2 stride,  outputs 14x14x30, as CONV1 \t\t\t\t    |\n",
    "| Convolution 5x5\t    | 1x1 stride, same padding, 64 filters, outputs 10x10x64    |\n",
    "| RELU          \t\t|         \t\t\t\t\t\t\t\t\t                |\n",
    "| Max pooling\t\t    | 2x2 stride,  outputs 5x5x64, as CONV2   \t                |\n",
    "| Fully connected\t\t| **CONV1 and CON2 were flatten together, 1 fully conneted layers.**\t|\n",
    "| RELU                  |                                                           |\n",
    "|Dropout                | Keep Probability = 0.3                                    |\n",
    "| Softmax\t\t\t\t| one hot encoded\t\t\t\t\t\t\t\t\t\t\t|\n",
    "\n",
    "#### 3. Describe how you trained your model.\n",
    "\n",
    "I used **SGD** as my approach to train the model and chose **AdamOptimizer** as the optimizer.\n",
    "In the training and experiment, my focus was find a good learning pace and also minimize overfitting. \n",
    "\n",
    "The parameters I tuned were **EPOCHS**, **BATCH_SIZE**, **learning rate** and **dropout**. **BATCH_SIZE**, **learning rate** were tuned to find a good leanrning pace, so that gradient desenct would not take too long or be stuck in a local optimal point or not converge. **dropout** was used to prevent overfitting.\n",
    "\n",
    "Here are the parameters:\n",
    "**EPOCHS** = 15\n",
    "**BATCH_SIZE** = 128\n",
    "**learning rate** = 0.001\n",
    "**dropout** = 0.7\n",
    "\n",
    "#### 4. Describe the approach taken for finding a solution and getting the validation set accuracy to be at least 0.93.\n",
    "After reading [Pierre Sermanet and Yann LeCun's Traffic Sign Recognition with Multi-Scale Convolutional Networks (http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf), I changed LeNet's architecture and adopted their Multi-scale Convolutional Networks, which, instead of only fully connets second convolutional result, combines the first layer's convolutional result.\n",
    "\n",
    "Then I tuned parameters like **EPOCHS**, **BATCH_SIZE**, **learning rate** and **dropout** to make validation set accuracy to be at least 0.93. The result is pretty good. Since the nature of SGD, the accuracies were oscillating, but overall **training accuracy is 99.8% and validation accuracy on average is 96.5% **.\n",
    "\n",
    "I noticed the training accuracy is 100%, which may be a sign of overfitting, but with **dropout = 0.7** and later's testing accuracy similar to validation accuracy, I believe the model is not overfitting.\n",
    "\n",
    "\n",
    "**Multi-Scale Convolutional Networks (MSCNN) architecture vs LeNet architecture**: *MSCNN's feeding both 1st and 2nd layers of convolutional result to the classifier provides different scales to the model. [The motivation for combining\n",
    "representation from multiple stages in the classifier is to\n",
    "provide different scales of receptive fields to the classifier.\n",
    "In the case of 2 stages of features, the second stage extracts\n",
    "“global” and invariant shapes and structures, while the first\n",
    "stage extracts “local” motifs with more precise details. We\n",
    "demonstrate the accuracy gain of using such layer-skipping\n",
    "connections in section III-B.] (http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf)*\n",
    "\n",
    "My final model results were:\n",
    "* training set accuracy of 100%\n",
    "* validation set accuracy of 95+%\n",
    "* test set accuracy of 95%\n",
    "\n",
    "Here is the training log:\n",
    "EPOCH 1 ...\n",
    "Training Accuracy = 0.871\n",
    "Validation Accuracy = 0.773\n",
    "\n",
    "EPOCH 2 ...\n",
    "Training Accuracy = 0.960\n",
    "Validation Accuracy = 0.884\n",
    "\n",
    "EPOCH 3 ...\n",
    "Training Accuracy = 0.978\n",
    "Validation Accuracy = 0.912\n",
    "\n",
    "EPOCH 4 ...\n",
    "Training Accuracy = 0.986\n",
    "Validation Accuracy = 0.933\n",
    "\n",
    "EPOCH 5 ...\n",
    "Training Accuracy = 0.992\n",
    "Validation Accuracy = 0.938\n",
    "\n",
    "EPOCH 6 ...\n",
    "Training Accuracy = 0.992\n",
    "Validation Accuracy = 0.919\n",
    "\n",
    "EPOCH 7 ...\n",
    "Training Accuracy = 0.996\n",
    "Validation Accuracy = 0.946\n",
    "\n",
    "EPOCH 8 ...\n",
    "Training Accuracy = 0.996\n",
    "Validation Accuracy = 0.945\n",
    "\n",
    "EPOCH 9 ...\n",
    "Training Accuracy = 0.997\n",
    "Validation Accuracy = 0.952\n",
    "\n",
    "EPOCH 10 ...\n",
    "Training Accuracy = 0.998\n",
    "Validation Accuracy = 0.950\n",
    "\n",
    "EPOCH 11 ...\n",
    "Training Accuracy = 0.998\n",
    "Validation Accuracy = 0.951\n",
    "\n",
    "EPOCH 12 ...\n",
    "Training Accuracy = 0.998\n",
    "Validation Accuracy = 0.954\n",
    "\n",
    "EPOCH 13 ...\n",
    "Training Accuracy = 0.998\n",
    "Validation Accuracy = 0.952\n",
    "\n",
    "EPOCH 14 ...\n",
    "Training Accuracy = 0.999\n",
    "Validation Accuracy = 0.961\n",
    "\n",
    "EPOCH 15 ...\n",
    "Training Accuracy = 0.998\n",
    "Validation Accuracy = 0.965\n",
    "\n",
    "### Test a Model on New Images\n",
    "[image8]: ./pictures_for_writeup/resize_Caution.jpg \"Caution\"\n",
    "[image9]: ./pictures_for_writeup/resize_PriorityRoad.jpg \"PriorityRoad\"\n",
    "[image10]: ./pictures_for_writeup/resize_stop.jpg \"stop\"\n",
    "[image11]: ./pictures_for_writeup/resize_30speed.jpg \"30speed\"\n",
    "[image12]: ./pictures_for_writeup/resize_Do-Not-Enter.jpg \"Do-Not-Enter\"\n",
    "[image13]: ./pictures_for_writeup/resize_roadWorks-2.jpg \"roadWorks\"\n",
    "\n",
    "\n",
    "Here are 6 newly downloaed pictures:\n",
    "![][image8]\n",
    "![][image9]\n",
    "![][image10]\n",
    "![][image11]\n",
    "![][image12]\n",
    "![][image13]\n",
    "\n",
    "I think the 4th and last are a little bit difficult to predict, since they have some background noises.\n",
    "\n",
    "#### 2. Discuss the model's predictions on these new traffic signs and compare the results to predicting on the test set.\n",
    "For the web downloaded testing set, the accuracy is 100%, compared to **testing data's 95%**. It is pretty normal, since our web testing data is too small (only 6).\n",
    "\n",
    "Here are the results of the predictions:\n",
    "\n",
    "| Image\t\t\t        |     Prediction\t        \t\t\t\t\t| \n",
    "|:---------------------:|:---------------------------------------------:| \n",
    "| Caution        \t\t| Caution   \t\t\t\t\t\t\t\t\t| \n",
    "| Priority Road     \t| Priority Road\t\t\t\t\t\t\t\t\t|\n",
    "| Stop Sign\t\t\t\t| Stop Sign\t\t\t\t\t\t\t\t\t\t|\n",
    "| 30 km/h limit\t      \t| 30 km/h limit\t\t\t\t\t \t\t\t\t|\n",
    "| Do-Not-Enter          |Do-Not-Enter|\n",
    "| Road Works\t\t\t| Road Works      \t\t\t\t\t\t\t    |\n",
    "\n",
    "As shown above, the model's predictions on those 6 new pictures are all correct.\n",
    "\n",
    "#### 3. Describe how certain the model is when predicting on each of the five new images by looking at the softmax probabilities for each prediction. Provide the top 5 softmax probabilities for each image along with the sign type of each probability.\n",
    "\n",
    "Here are the top 5 softmax probabilities for the 6 newly downloaded pictures:\n",
    "\n",
    "|  Top 1 Probability         \t|     Prediction\t        \t\t\t\t\t| \n",
    "|:---------------------:|:---------------------------------------------:| \n",
    "| 1.00         \t\t\t| Caution  \t\t\t\t\t\t\t\t\t    | \n",
    "| 1.00     \t\t\t\t| Priority Road\t\t\t\t\t\t\t\t\t|\n",
    "| 1.00\t\t\t\t\t| Stop Sign\t\t\t\t\t\t\t\t\t\t|\n",
    "| 0.557\t      \t\t\t| 30 km/h limit\t\t\t\t\t\t \t\t\t|\n",
    "| 1.00\t\t\t\t    | Do-Not-Enter     \t\t\t\t\t\t\t    |\n",
    "| 0.991                 | Road Works                                    |\n",
    "Here are the pie charts showing relative probabilities distribution:\n",
    "[image14]: ./pictures_for_writeup/picture_0.jpg \"picture1\"\n",
    "[image15]: ./pictures_for_writeup/picture_1.jpg \"picture2\"\n",
    "[image16]: ./pictures_for_writeup/picture_2.jpg \"picture3\"\n",
    "[image17]: ./pictures_for_writeup/picture_3.jpg \"picture4\"\n",
    "[image18]: ./pictures_for_writeup/picture_4.jpg \"picture5\"\n",
    "[image19]: ./pictures_for_writeup/picture_5.jpg \"picture6\"\n",
    "\n",
    "![][image14]\n",
    "![][image15]\n",
    "![][image16]\n",
    "![][image17]\n",
    "![][image18]\n",
    "![][image19]\n",
    "\n",
    "**We can see here, compared to Picture 1,2,3 and 5, the predictions for Picture 4 and 5 are not that certain, especially Picture 4.**\n",
    "\n",
    "### (Optional) Visualizing the Neural Network (See Step 4 of the Ipython notebook for more details)\n",
    "\n",
    "Example I used \"Caution Sign\" as an example.\n",
    "Processed Image\n",
    "[image20]: ./pictures_for_writeup/visualization1.jpg \"visualization1\"\n",
    "![][image20]\n",
    "\n",
    "After being converted to grayscale and normalized\n",
    "[image21]: ./pictures_for_writeup/visualization2.jpg \"visualization2\"\n",
    "![][image21]\n",
    "\n",
    "First convolutional layer 30 filters\n",
    "[image22]: ./pictures_for_writeup/First_layer_filters.png \"1st layer\"\n",
    "![][image22]\n",
    "Second convolutional layer 64 filters\n",
    "[image23]: ./pictures_for_writeup/Second_layer_filters.png \"2nd layer\"\n",
    "![][image23]\n",
    "\n",
    "**As we can see the first layer seems to find the general shape and second layer is trying to identify certain pattern.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
